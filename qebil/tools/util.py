import hashlib
from os import makedirs, path, remove
import pandas as pd
import numpy as np
import PyPDF2
import re
import requests
from shutil import move
import urllib

from qebil.tools.fastq import get_read_length

from qebil.log import logger

THIS_DIR, THIS_FILENAME = path.split(__file__)


def load_project_file(filepath):
    """Reads in a project file to populate a list of study ids

    This method looks for a 'study_id' column in a tsv-formatted
    text file, as generated by the search function of QEBIL, and
    checks the first column if not found. This latter catch allows
    the user to supply a list of project ids, one per line to fetch.

    Parameters
    ----------
    filepath: string:
        path to the project file with a 'study_id' column

    Returns
    ---------
    add_list: list
        list of EBI project IDs found in the file

    """
    proj_df = pd.read_csv(filepath, sep="\t", header=0)
    if "study_id" not in proj_df.columns:
        proj_df = pd.read_csv(filepath, sep="\t", header=None)
        proj_df.columns = ["study_id"]
    add_list = list(proj_df["study_id"].unique())
    add_list = [x.strip() for x in add_list]

    return add_list


def compare_checksum(filepath, md5_value):
    """Compares the expected and actual checksum of the file

    This function compares the expected md5 checksum with the md5
    checksum from the specified local file.
    Returns False if they do not match or the checksum
    value if they do.

    Parameters
    ----------
    filepath: string
        the path to the file or URL to be scraped

    Returns
    ---------
    False OR local_checksum:string
        checksum of local file if valid, otherwise False
    """

    if path.isfile(filepath):
        fq = open(filepath, "rb")
        fq_contents = fq.read()
        fq.close()
        local_checksum = hashlib.md5(fq_contents).hexdigest()

        if md5_value != local_checksum:
            return False
        else:
            return local_checksum
    else:
        logger.warning(filepath + " not found.")
        return False


def parse_document(filepath):
    """This method allows local files or URLs to be parsed

    Uses basic NLP to parse a html page or file (PDF or text)
    for its contents

    Parameters
    ----------
    filepath: string
        the path to the file or URL to be scraped

    Returns:
    ----------
    tokens: list
        list of parsed tokens
    """
    full_text = ""
    tokens = []

    if filepath[0:3] == "10.1":
        # this is a doi, so just prepend and go with it
        filepath = "https://doi.org/" + filepath

    if filepath[0:3] == "htt":
        request = requests.get(filepath)
        if request.status_code == 200:
            if filepath[-3:] == "pdf":
                file_name, headers = urllib.request.urlretrieve(filepath)
                document = PyPDF2.PdfFileReader(open(file_name, "rb"))
                for i in range(document.numPages):
                    page_to_print = document.getPage(i)
                    full_text += page_to_print.extractText()
                tokens = [
                    t for t in re.split(r"\; |\, |\. | |\n|\t", full_text)
                ]
            else:
                full_text = request.text
                tokens = [
                    t
                    for t in re.split(
                        r"\;|\,|\.| |\n|\t|<|>|\/|\"|'", full_text
                    )
                ]
    else:
        if filepath[-3:] == "pdf":
            document = PyPDF2.PdfFileReader(open(filepath, "rb"))
            for i in range(document.numPages):
                page_to_print = document.getPage(i)
                full_text += page_to_print.extractTexit()
            tokens = [t for t in re.split(r"\; |\, |\. | |\n|\t", full_text)]
        else:
            text_file = open(filepath, "r")
            full_text = text_file.read()
            text_file.close()
            tokens = [
                t
                for t in re.split(r"\;|\,|\.| |\n|\t|<|>|\/|\"|'", full_text)
            ]

    return tokens


def scrape_ebi_ids(tokens, proj_id_stems=["PRJEB", "PRJNA", "ERP", "SRP"]):
    """This method allows local files or URLs to be stripped for
    EBI/ENA project IDs using tokenization

    Parameters
    ----------
    tokens: list
        list of parsed tokens from a file or webpage
    proj_id_stems: list
        list of expected project prefixes to look for

    Returns
    ---------
    found_ebi_ids : list
        list of EBI IDs found in the document
    """

    found_ebi_ids = []

    potential_tokens = {
        stem: t for stem in proj_id_stems for t in tokens if stem in t
    }
    potential_tokens = list(potential_tokens.values())

    for t in potential_tokens:
        for stem in proj_id_stems:
            # locate the start of the prefix within the token if present
            start = t.find(stem)  # will return -1 if not found

            if start != -1 and len(t) >= start + len(stem) + 1:
                # if a prefix is found, see if the next character is a number
                next_char = t[start + len(stem)]
                if next_char.isnumeric():
                    test_study = t[
                        start:
                    ]  # for now, assume that the token contains the ID
                    ebi_url = (
                        "https://www.ebi.ac.uk/ena/browser/view/"
                        + test_study
                        + "&display=xml"
                    )
                    request2 = requests.get(ebi_url)
                    invalid = True
                    if request2.status_code == 200:
                        if len(test_study) >= 6:  # min size filter
                            found_study = (
                                test_study.strip()
                                .strip(",")
                                .strip(".")
                                .strip("-")
                            )
                            found_ebi_ids.append(found_study)
                            invalid = False
                    if invalid:
                        logger.warning(
                            "Found stem: "
                            + test_study
                            + " in "
                            + test_study
                            + " but EBI project URL "
                            + ebi_url
                            + " does not exist."
                        )

    return found_ebi_ids


def get_ebi_ids(study_xml_dict):
    """Simple method to retrieve the paired study and project accessions

    If the the EBI accessions are not found, will return a tuple of
    False, False

    Parameters
    -----------
    study_xml_dict: dict
        details retrieved from fetch_ebi_info

    Returns
    ----------
    study_accession, proj_accession: (string, string) or (False, False)
        stud and project accessions if found
    """
    # TODO: does not catch case where STUDY_SET and PROJECT_SET in keys
    # TODO: consider refactor with xml object instead of dict

    study_accession = False
    proj_accession = False

    if "STUDY_SET" in study_xml_dict:
        id_dict = study_xml_dict["STUDY_SET"]["STUDY"]["IDENTIFIERS"]
        study_accession = id_dict["PRIMARY_ID"]
        if "SECONDARY_ID" in id_dict:
            proj_accession = id_dict["SECONDARY_ID"]
            # need to catch for the case when there are two secondary IDs...
            # all project accessions should start with PRJ
            if type(proj_accession) == list:
                proj_accession = "".join(
                    [p for p in proj_accession if "PRJ" in p]
                )
        else:
            logger.warning("No project ID for study: " + study_accession)
            proj_accession = study_accession
    elif "PROJECT_SET" in study_xml_dict:
        id_dict = study_xml_dict["PROJECT_SET"]["PROJECT"]["IDENTIFIERS"]
        proj_accession = id_dict["PRIMARY_ID"]
        if "SECONDARY_ID" in id_dict:
            study_accession = id_dict["SECONDARY_ID"]
        else:
            logger.warning("No study ID for project: " + proj_accession)
    else:
        logger.warning("No study information found.")

    return study_accession, proj_accession


def setup_output_dir(output_dir):
    """Helper function to ensure output path exists and is valid"""

    # output_dir directory
    if output_dir[-1] != "/":
        output_dir += "/"

    if not path.exists(output_dir):
        makedirs(output_dir, exist_ok=True)  # exist_ok shouldn't be an issue?

    return output_dir


def detect_qiita_study(metadata):
    """Simple helper function to catch when a study is already in Qiita"""
    if "qiita_study_id" in metadata.columns:
        return metadata["qiita_study_id"].unique()
    else:
        return False


def parse_details(xml_dict, null_val="XXEBIXX"):
    """Method to parse study details from xml dict

    Parameters
    -----------
    study_details: dict
        dict of study details parsed from xml

    Returns
    ----------
    desc_dict: dict
        parsed details

    """

    result_dict = {
        "abstract": null_val,
        "description": null_val,
        "title": null_val,
        "seq_method": [],
    }

    parse_dict = xml_dict["STUDY_SET"]["STUDY"]
    if "DESCRIPTOR" not in parse_dict.keys():
        logger.warning(
            "No DESCRIPTOR values found. Using " + null_val + " for values."
        )
        return result_dict
    else:
        desc_dict = parse_dict["DESCRIPTOR"]

    if len(desc_dict) > 0:
        if "STUDY_ABSTRACT" in desc_dict.keys():
            result_dict["abstract"] = desc_dict["STUDY_ABSTRACT"]

        elif "ABSTRACT" in desc_dict.keys():
            result_dict["abstract"] = desc_dict["ABSTRACT"]
        else:
            logger.warning(
                "No abstract found, using " + null_val + " for abstract"
            )

        if "STUDY_DESCRIPTION" in desc_dict.keys():
            result_dict["description"] = desc_dict["STUDY_DESCRIPTION"]
        elif "DESCRIPTION" in desc_dict.keys():
            result_dict["description"] = desc_dict["DESCRIPTION"]
        else:
            logger.warning(
                "No description found, using " + null_val + " for description"
            )

        if "@alias" in parse_dict.keys():
            alias = parse_dict["@alias"]
            logger.warning(
                "Found EBI alias, appending '" + alias + "' to description"
            )
            if result_dict["description"] == null_val:
                result_dict["description"] = alias
            else:
                result_dict["description"] += alias

        if "STUDY_TITLE" in desc_dict.keys():
            result_dict["title"] = desc_dict["STUDY_TITLE"]
        elif "TITLE" in desc_dict.keys():
            result_dict["title"] = desc_dict["TITLE"]
        else:
            logger.warning("No title found, using " + null_val + " for title")

        for k in result_dict.keys():
            if result_dict[k] != null_val:
                result_dict["seq_method"] += scrape_seq_method(
                    result_dict["description"]
                )

    return result_dict


def scrape_seq_method(study_text):
    """Method to search text for relevant sequencing methods"""
    valid_methods = [
        "16s",
        "18s",
        "its1",
        "its2",
        "shotgun",
    ]
    tokens = [t for t in re.split(r"\; |\, |\. | |\n|\t", study_text.lower())]
    found_methods = [t for meth in valid_methods for t in tokens if meth in t]
    return found_methods


def unpack_fastq_ftp(fastq_ftp, fastq_md5, sep=";"):
    """Unpacks the ftp and md5 field from EBI metadata

    Takes paired set of ebi-format fastq_ftp and fastq_md5
    string values and parses them into a dictionary to be used for
    downloading and using a checksum to validate the downloads

    Parameters
    ----------
    fastq_ftp: string
        string of semicolon separated ftp filepaths
    fastq_md5: string
        string of semicolon separated md5 checksums

    Returns
    ----------
    remote_dict: dict
        dict of paired ftp filepaths and md5 checksums
    """
    remote_dict = {}
    ftp_list = fastq_ftp.split(sep)
    logger.info(ftp_list)
    md5_list = fastq_md5.split(sep)

    if len(ftp_list) == 0:
        error_msg = (
            "No ftp files present. Check study details"
            + " as access may be restricted"
        )
        logger.warning(error_msg)
    else:
        read_counter = 0
        while read_counter < len(ftp_list):
            read_dict = {}
            read_dict["ftp"] = ftp_list[read_counter]
            read_dict["md5"] = md5_list[read_counter]
            read_counter += 1
            remote_dict["read_" + str(read_counter)] = read_dict

    return remote_dict


def remove_index_read_file(fastq_dict, layout):
    """Simple method to quickly remove and rename files to
    handle samples with index files

    Parameters
    ----------
    file_prefix: string
        file_prefix to glob

    Returns
    ----------
    None

    """
    new_fastq_dict = fastq_dict
    file_prefix = (
        fastq_dict["read1"].split("/")[-1].replace(".R1.ebi.fastq.gz", "")
    )
    if layout.lower() == "single":
        expected_files = 1
    elif layout.lower() == "paired":
        expected_files = 2
    else:
        logger.warning(
            "layout: "
            + str(layout)
            + " not valid for removing index read file."
        )

    if len(fastq_dict) == expected_files:
        logger.info(
            "Expected file count matches found file count for prefix "
            + file_prefix
            + ". Skipping index read removal."
        )
    elif len(fastq_dict) <= 3:
        read_length_dict = {}
        for f in fastq_dict.keys():
            read_length_dict[f] = get_read_length(fastq_dict[f])

        read_lengths = list(read_length_dict.values())
        if int(np.min(read_lengths)) >= int(np.mean(read_lengths)) / 2:
            logger.warning(
                "Minimum and mean read lengths are less than 2-fold different."
                + " Could not identify index read file for"
                + file_prefix
            )
        else:
            index_file = min(read_length_dict, key=read_length_dict.get)
            logger.info(
                index_file
                + " identified as index for "
                + file_prefix
                + " Removing and renaming other file(s)."
            )
            remove(fastq_dict[index_file])
            index_read_num = index_file[-1]
            if int(index_read_num) == 1:
                # typically read1 is expected to be the index read if present
                move(fastq_dict["read2"], fastq_dict[index_file])
                new_fastq_dict["read1"] = fastq_dict[index_file]
                if "read3" in fastq_dict.keys():
                    move(fastq_dict["read3"], fastq_dict["read2"])
                    del new_fastq_dict["read3"]
            elif "read3" in fastq_dict.keys():
                # if read2 is removed and there were only two, no action
                # otherwise move read3 to read 2
                move(fastq_dict["read3"], fastq_dict["read2"])
                del new_fastq_dict["read3"]
    else:
        logger.warning(
            "Removing index read files not implemented for >3 reads."
        )
        # TODO: handle samples with 4 reads?

    return new_fastq_dict
